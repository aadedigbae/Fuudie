{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujuLMxSvoXbg"
      },
      "source": [
        "# Optimization Techniques in Machine Learning\n",
        "\n",
        "Objective: This assignment aims to explore implementation or Machine Learning Models with regularization, optimization and Error analysisÂ  techniques used in machine learning to improve models' performance, convergence speed, and efficiency..\n",
        "\n",
        "A Notebook detailing the following\n",
        "\n",
        "* Project name\n",
        "* Clear out puts from cells"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8VW_IzbI3od"
      },
      "source": [
        "\n",
        "# Case Study and Implementation\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import Necessary Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, precision_score, recall_score, roc_auc_score\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, load_model, save_model\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import xgboost as xgb\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nas-T7xwPIso"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fulfillment Center Info:\n",
            "   center_id  city_code  region_code center_type  op_area\n",
            "0         11        679           56      TYPE_A      3.7\n",
            "1         13        590           56      TYPE_B      6.7\n",
            "2        124        590           56      TYPE_C      4.0\n",
            "3         66        648           34      TYPE_A      4.1\n",
            "4         94        632           34      TYPE_C      3.6\n",
            "\n",
            "Meal Info:\n",
            "   meal_id   category cuisine\n",
            "0     1885  Beverages    Thai\n",
            "1     1993  Beverages    Thai\n",
            "2     2539  Beverages    Thai\n",
            "3     1248  Beverages  Indian\n",
            "4     2631  Beverages  Indian\n",
            "\n",
            "Train Data:\n",
            "        id  week  center_id  meal_id  checkout_price  base_price  \\\n",
            "0  1379560     1         55     1885          136.83      152.29   \n",
            "1  1466964     1         55     1993          136.83      135.83   \n",
            "2  1346989     1         55     2539          134.86      135.86   \n",
            "3  1338232     1         55     2139          339.50      437.53   \n",
            "4  1448490     1         55     2631          243.50      242.50   \n",
            "\n",
            "   emailer_for_promotion  homepage_featured  num_orders  \n",
            "0                      0                  0         177  \n",
            "1                      0                  0         270  \n",
            "2                      0                  0         189  \n",
            "3                      0                  0          54  \n",
            "4                      0                  0          40  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load datasets\n",
        "fulfillment_data = pd.read_csv('Dataset/fulfilment_center_info.csv')\n",
        "meal_data = pd.read_csv('Dataset/meal_info.csv')\n",
        "train_data = pd.read_csv('Dataset/train.csv')\n",
        "\n",
        "# Display basic information\n",
        "print(\"Fulfillment Center Info:\")\n",
        "print(fulfillment_data.head())\n",
        "\n",
        "print(\"\\nMeal Info:\")\n",
        "print(meal_data.head())\n",
        "\n",
        "print(\"\\nTrain Data:\")\n",
        "print(train_data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge datasets\n",
        "merged_data = train_data.merge(fulfillment_data, on='center_id', how='left')\n",
        "merged_data = merged_data.merge(meal_data, on='meal_id', how='left')\n",
        "\n",
        "# Handle missing values (if any)\n",
        "merged_data.fillna(0, inplace=True)\n",
        "\n",
        "# Encode categorical variables\n",
        "merged_data = pd.get_dummies(merged_data, columns=['center_type', 'category', 'cuisine'], drop_first=True)\n",
        "\n",
        "# Define features and target\n",
        "X = merged_data.drop(columns=['id', 'num_orders'])  # 'num_orders' is the target column\n",
        "y = merged_data['num_orders']\n",
        "\n",
        "# Standardize numerical features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split into training, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "KeyboardInterrupt\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Train Logistic Regression\n",
        "log_reg = LogisticRegression(max_iter=1000)\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate on validation set\n",
        "y_pred = log_reg.predict(X_val)\n",
        "print(\"Logistic Regression Accuracy:\", accuracy_score(y_val, y_pred))\n",
        "print(classification_report(y_val, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train SVM\n",
        "svm_model = SVC(kernel='rbf', C=1.0)\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate on validation set\n",
        "y_pred_svm = svm_model.predict(X_val)\n",
        "print(\"SVM Accuracy:\", accuracy_score(y_val, y_pred_svm))\n",
        "print(classification_report(y_val, y_pred_svm))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nr6HuVEtXvoP"
      },
      "outputs": [],
      "source": [
        "#TODO:\n",
        "model_2 = define_model('Adam', None)\n",
        "loss_curve_plot(model_2):\n",
        "#print out confusion matrix and error analysis metrics after the cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UygzqjB-Xvgo"
      },
      "outputs": [],
      "source": [
        "#TODO:\n",
        "model_3 = define_model('RMSPop',None)\n",
        "loss_curve_plot(model_3):\n",
        "#print out confusion matrix and error analysis metrics after the cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NEtnjqxXvXm"
      },
      "outputs": [],
      "source": [
        "#TODO:\n",
        "model_4 = define_model(None)\n",
        "loss_curve_plot(model_4):\n",
        "#print out confusion matrix and error analysis metrics after the cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aZLEriYOXI1"
      },
      "source": [
        "#Task: Make Predictions using the best saved model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnBLwqTJX3p0"
      },
      "source": [
        "Create a confusion Matrix and F1 score for both Models. Ensure outputs for the cells are visible"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bO79SOsZYG-M"
      },
      "source": [
        "Finally, Make predictions using the best model. By the time you get to this cell you may realise at some point you needed to save the model so that you cal load it later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nqqe2PasUIAG"
      },
      "outputs": [],
      "source": [
        "def make_predictions(model_path, X):\n",
        "\n",
        "    # Load the model\n",
        "    model = load_model(None)\n",
        "    # Make predictions\n",
        "    predictions = None\n",
        "    # Convert probabilities to binary labels (0 or 1)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "#Modify the code appropriately"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_jwbvaAUMj4"
      },
      "outputs": [],
      "source": [
        "model_path = None\n",
        "make_predictions(None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfTHk2nZMzTH"
      },
      "source": [
        "Congratulations!!\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
